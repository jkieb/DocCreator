# Code-ErklÃ¤rer

Ein **ultra-einfaches, lineares** Python-Skript, das deinen Python-Quellcode in **logische Segmente** aufteilt und **jedes Segment von der ChatGPT-API erklÃ¤ren lÃ¤sst**. Ergebnis ist ein **sauber formatiertes Markdown-Dokument** mit Inhaltsverzeichnis.

## Features

* ðŸ”€ **Zwei Schritte in einem Lauf**:

  1. Segmentierung des gesamten Codes â†’ JSON
  2. ErklÃ¤rung jedes Segments â†’ Markdown-Kapitel
* ðŸ§­ **Inhaltsverzeichnis** mit Anker-Links
* ðŸ§± **Kompakte ErklÃ¤rungen** mit Abschnitten: ErklÃ¤rung, Besonderheiten/RandfÃ¤lle, Hinweise, Schnittstellen
* ðŸ§© **Modell & Temperatur per ENV** konfigurierbar
* ðŸ“ **Einfacher Output**: eine einzige `.md`-Datei

## Schnellstart

```bash
# 1) AbhÃ¤ngigkeiten
pip install openai python-dotenv

# 2) API-Key setzen (oder per .env)
export OPENAI_API_KEY="sk-..."

# optional: Modell wÃ¤hlen
export OPENAI_MODEL="gpt-4o-mini"
export OPENAI_TEMPERATURE="0.2"

# 3) Skript ausfÃ¼hren
python segment_and_explain_linear.py path/to/your_code.py path/to/output.md
```

* **Input:** Eine Python-Datei (`your_code.py`)
* **Output:** Ein Markdown-Dokument (`output.md`) mit ErklÃ¤rungen pro Segment

## Ablauf (Ã¼berblick)

```mermaid
flowchart LR
    A[Python-Datei] --> B[Segmentierung]
    B --> C[JSON]
    C --> D[Doku erstellen]
    D --> E[Markdown]
    E --> F[output.md]
```

## CLI

```bash
python segment_and_explain_linear.py <pfad_zur_datei.py> <output.md>
```

* Beendet sich mit Hinweis auf den Zielpfad, wenn erfolgreich.
* LÃ¤uft strikt linear (keine Nebenprozesse, kein Retry-Mechanismus).

## Konfiguration

Ãœber Umgebungsvariablen oder `.env`:

* `OPENAI_API_KEY` **(Pflicht)** â€“ dein API-Key
* `OPENAI_MODEL` *(optional)* â€“ z. B. `gpt-4o-mini` (Default)
* `OPENAI_TEMPERATURE` *(optional)* â€“ z. B. `0.2` (prÃ¤ziser Stil)

Beispiel `.env`:

```ini
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.2
```

## Output-Struktur

`output.md` enthÃ¤lt:

* `# ErklÃ¤rung des Codes` (Header + kurze Einleitung)
* `## Inhalt` (TOC mit Anker-Links)
* FÃ¼r **jedes Segment**:

  * `### {n}. {Titel}`
  * ein kleiner Codeblock-Platzhalter
  * **ErklÃ¤rung**
  * **Besonderheiten & RandfÃ¤lle**
  * **Hinweise**
  * **Schnittstellen**
* Footer mit Zeitstempel, Modell, Temperatur und Segmentanzahl

## Performance

* Es werden **9 API-Calls** bei 8 Segmenten (1Ã— Segmentierung + 8Ã— ErklÃ¤rung) ausgelÃ¶st.
* Laufzeit â‰ˆ *Summe der API-Antwortzeiten*. FÃ¼r `gpt-4o-mini` hÃ¤ufig **~10â€“20 s**, je nach CodegrÃ¶ÃŸe/Segmentanzahl auch lÃ¤nger.

## Sicherheit / Kosten

* **API-Key nicht commiten.** Nutze `.env` und fÃ¼ge sie zu `.gitignore` hinzu.
* Jeder Aufruf erzeugt **Token-Kosten**. KÃ¼rzere Segmente und geringere `max_tokens` reduzieren Kosten.
* EnthÃ¤lt dein Code **sensible Daten**, prÃ¼fe vor dem Senden an die API.

## Ordnerstruktur (Vorschlag)

```
.
â”œâ”€ segment_and_explain_linear.py
â”œâ”€ README.md
â”œâ”€ .env                 # niemals committen
â””â”€ examples/
   â”œâ”€ demo_input.py
   â””â”€ demo_output.md
```

## Typische Fragen (FAQ)

**Warum zwei Schritte?**
Segmentierung und ErklÃ¤rung sind unterschiedliche Aufgaben mit unterschiedlichen Prompts. So bleiben die ErklÃ¤rungen pro Segment prÃ¤zise und referenzieren den genauen Code-Block.

**Kann ich das Modell Ã¤ndern?**
Ja, per `OPENAI_MODEL`. FÃ¼r Geschwindigkeit kannst du Varianten wie `gpt-4o-mini-fast` testen.

**Kann ich Zeit/Token sparen?**
Ja â€“ kÃ¼rzere Segmente, konservative `max_tokens` in den API-Calls (im Skript auf ~900 gesetzt), und ggf. weniger Detailtiefe im Prompt.

**Was, wenn die Segmentierung mal ungenau ist?**
Da das Skript linear und minimal ist (ohne Error-Handling), passe ggf. den Segmentierungs-Prompt im Skript leicht an (z. B. strengere Segment-Hinweise).

**Gibt es eine Beispielausgabe?**
[`erklaerung.md`](erklaerung.md)
## Lizenz

Auf Anfrage

